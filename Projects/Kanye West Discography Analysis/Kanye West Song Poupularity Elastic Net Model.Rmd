---
title: "Kanye West Song Popularity Elastic Net Model"
output: html_document
date: '2022-07-06'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this data exploration, we look at how various song attributes such as dance ability and acousticness affect song popularity, specifically within the catalog of Kanye West. Track attribute data was obtained through the Spotify API.

This analysis consists of the creation of an elastic net model as a way to develop a regression model to predict a track's popularity (rated from 0 to 100) based on predictors such as dance ability, tempo, and duration. An elastic net model for this scenario will enable us to examine the importance of each song attribute as it relate to popularity, and due to a relatively small sample size, the elastic net method will prevent the model from over fitting to the training set.

Now, we will explore the analysis!

```{r}
library(caret) # load in relevant packages and tools
library(glmnet)
library(MASS)
library(tidyverse)
library(ggplot2)

# read in csv from file location
df <- read.csv("C:/Users/M33066/OneDrive - Noblis/kanye_catalog_track_attributes.csv")
data <- df[-1] # remove the song title as it is a non-numeric attribute that cannot be used in the modeling

head(data)
```

The data frame view shows some of the track attributes which are rated on numeric scales. Examine the summary statistics for Mr. West's catalog for further information about the track attributes.

```{r}
summary(data)
```

The following section shows the random partitioning of the track data frame into training and testing data sets. Our model will be created with the training data, and we will assess its performance by applying it to the testing data.

```{r}
set.seed(47) #reproducible results

# createDataPartition() function from the caret package to split the original data set into a training and testing set and split data into training (80%) and testing set (20%)
parts = createDataPartition(data$popularity, p = .8, list = F)
train = data[parts, ]
test = data[-parts, ]

# specifying the CV technique which will be passed into the train() function later and number parameter is the "k" in K-fold cross validation
train_control = trainControl(method = "cv", number = 10, search = "grid")
```


## Model Creation

Now, we get into some model creation, exploring multiple options in order to maximize the accuracy of the models (maximizing r-squared value and minimizing RMSE)

Model1 shows a prelininary look at some possible elastic Net models for predicting track popularity. This is the selected parameterization of an elastic net model that best fits the training data.

```{r}
# training a Elastic Net Regression model while tuning parameters
# set to minimize both RMSE and R-squared by default
model1 = train(
  popularity~., 
  data = train, 
  method = "glmnet", 
  trControl = train_control
)

plot(model1)
#mixing percentage: alpha
#RMSE: root mean squared error
#Regularization Parameter: lambda

# summarizing the results
model1$bestTune
```

Even though the caret package makes it easy for us to discover our optimal model, lets look at what happens with other parameter combinations.

In order to get a better understanding of the behavior of our alpha and lambda parameters in the model, we enact a manual grid search of parameters to test many more parameter combinations, evaluating them on their performance metrics.

```{r}
# Manual parameter grid searches for a better parameter search
lambda.grid <- seq(0,10,length = 50)
alpha.grid <- seq(0,1, length = 11)

srchGrd = expand.grid(.alpha = alpha.grid, .lambda = lambda.grid)

model2 <- train(
  popularity~.,
  data = train,
  method = 'glmnet',
  tuneGrid = srchGrd,
  trControl = train_control
)

plot(model2)

model2$bestTune
```

Model2 allows a deeper look into the parameterization of predicting popularity. Notice how the parameters in Model2 are different than in Model1, and they are producing similar RMSE which can be seen on the graph. Due to the continual decline of the RMSE in the plot of Model2, We will look a a third model in order to see what impact larger lambda values may have on the performance metrics of the model.

```{r}
# More tuning to find ideal parameters
lambda.grid <- seq(0,100,length = 200)
alpha.grid <- seq(0,1, length = 11)

srchGrd = expand.grid(.alpha = alpha.grid, .lambda = lambda.grid)

model3 <- train(
  popularity~.,
  data = train,
  method = 'glmnet',
  tuneGrid = srchGrd,
  trControl = train_control
)

plot(model3)

model3$bestTune
```

Sure enough, we see another change in the parameterization. Before looking at the model performance on the test data, let's examine how the models did in regard to fitting the training data.

## Model Performance

```{r}
# Evaluating model performance on TRAIN data

#################### MODEL 1
print('MODEL 1')
pred_y = predict(model1, train)

# performance metrics on the test data
test_y = train[, 1]
print(paste0('MSE: ',mean((test_y - pred_y)^2))) #mse - Mean Squared Error
print(paste0('RMSE: ',caret::RMSE(test_y, pred_y))) #rmse - Root Mean Squared Error

#################### MODEL 2
print('MODEL 2')
pred_y = predict(model2, train)

# performance metrics on the test data
test_y = train[, 1]
print(paste0('MSE: ',mean((test_y - pred_y)^2))) #mse - Mean Squared Error
print(paste0('RMSE: ',caret::RMSE(test_y, pred_y))) #rmse - Root Mean Squared Error

#################### MODEL 3
print('MODEL 3')
pred_y = predict(model3, train)

# performance metrics on the test data
test_y = train[, 1]
print(paste0('MSE: ',mean((test_y - pred_y)^2))) #mse - Mean Squared Error
print(paste0('RMSE: ',caret::RMSE(test_y, pred_y))) #rmse - Root Mean Squared Error
```

We can see that as expected Model1 has the lowest RMSE, but let's see how they did on the test data.

```{r}
# Evaluating model performance on test data

#################### MODEL 1
print('MODEL 1')
pred_y = predict(model1, test)

# performance metrics on the test data
test_y = test[, 1]
print(paste0('MSE: ',mean((test_y - pred_y)^2))) #mse - Mean Squared Error
print(paste0('RMSE: ',caret::RMSE(test_y, pred_y))) #rmse - Root Mean Squared Error

#################### MODEL 2
print('MODEL 2')
pred_y = predict(model2, test)

# performance metrics on the test data
test_y = test[, 1]
print(paste0('MSE: ',mean((test_y - pred_y)^2))) #mse - Mean Squared Error
print(paste0('RMSE: ',caret::RMSE(test_y, pred_y))) #rmse - Root Mean Squared Error

#################### MODEL 3
print('MODEL 3')
pred_y = predict(model3, test)

# performance metrics on the test data
test_y = test[, 1]
print(paste0('MSE: ',mean((test_y - pred_y)^2))) #mse - Mean Squared Error
print(paste0('RMSE: ',caret::RMSE(test_y, pred_y))) #rmse - Root Mean Squared Error
```

It looks as if Model2 actually performed the best, though all did fairly similar in terms of their performance metrics. Looking at a simple linear regression model, the elastic net models performed much better in fitting the testing data.


```{r}
model <-lm(popularity~., data = train)

pred_y = predict(model, test)

# performance metrics on the test data
test_y = test[, 1]
print(paste0('MSE: ',mean((test_y - pred_y)^2))) #mse - Mean Squared Error
print(paste0('RMSE: ',caret::RMSE(test_y, pred_y))) #rmse - Root Mean Squared Error
```

The coefficients of the models denote how changes in various song attributes impact expected popularity of a track. For example, a 1 unit increase in danceability anticipates a 16.896 unit increase in popularity based on the projections from Model1.

```{r}
# coefficients
data.frame(elastic_net = as.data.frame.matrix(coef(model1$finalModel, model1$finalModel$lambdaOpt)))
```

An interesting thing to look at is the predictor importance in our model. A cool thing about elastic net models is that they tune theior behavior based on the importance of certain predictors. Seen below, danceability has the greates impact on popularity of all the track attributes.

```{r}
#Variable importance visual
plot(varImp(model1,scale=TRUE))
```



